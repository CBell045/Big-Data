{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "861cdbc9-bd8b-48fa-ad0f-b646c7a5b97b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Pipelines\n",
    "#### Silicon Slayers\n",
    "\n",
    "Source: [Databricks](https://docs.databricks.com/en/getting-started/data-pipeline-get-started.html#:~:text=A%20data%20pipeline%20implements%20the,data%20that%20users%20can%20consume)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4423fa76-3986-4f85-93ab-8cbc1affbcf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Before we begin\n",
    "Get your cluster up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2359d0-b29d-4afa-b956-44aff07a8900",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What is a data pipeline? \n",
    "Data pipelines are a process for moving data from sources (like a UI/Application) to targets (like a data warehouse). \n",
    "\n",
    "Data is extracted from the source, \"flows\" through the pipeline and neccesary transformations, and is loaded into the target. \n",
    "\n",
    "A key feature of data pipelines is automation; data can be loaded in batches or in (near) real time. \n",
    "\n",
    "### What is ETL? \n",
    "A common abbreviation is ETL, which stands for Extract, Transform, Load. ELT is also becoming more popular with cloud warehouses. \n",
    "\n",
    "### What are common ETL Tools? \n",
    "- Apache (Airflow, Kafka, Spark)  \n",
    "- dbt (Data Build Tool)  \n",
    "- Informatica  \n",
    "\n",
    "- AWS Glue  \n",
    "- Google Cloud Dataflow  \n",
    "- Microsoft Azure Data Factory  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c4c710-31d8-487d-bf90-eaaf15b2a81c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: <pyspark.sql.streaming.query.StreamingQuery at 0x7fc515071ca0>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n",
    "\n",
    "# Define variables used in the code below\n",
    "file_path = \"/databricks-datasets/songs/data-001/\"\n",
    "table_name = \"raw_song_data\"\n",
    "checkpoint_path = \"/tmp/pipeline_get_started/_checkpoint/song_data\"\n",
    "\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"artist_id\", StringType(), True),\n",
    "    StructField(\"artist_lat\", DoubleType(), True),\n",
    "    StructField(\"artist_long\", DoubleType(), True),\n",
    "    StructField(\"artist_location\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"end_of_fade_in\", DoubleType(), True),\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"key_confidence\", DoubleType(), True),\n",
    "    StructField(\"loudness\", DoubleType(), True),\n",
    "    StructField(\"release\", StringType(), True),\n",
    "    StructField(\"song_hotnes\", DoubleType(), True),\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"start_of_fade_out\", DoubleType(), True),\n",
    "    StructField(\"tempo\", DoubleType(), True),\n",
    "    StructField(\"time_signature\", DoubleType(), True),\n",
    "    StructField(\"time_signature_confidence\", DoubleType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"partial_sequence\", IntegerType(), True)\n",
    "  ]\n",
    ")\n",
    "\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .schema(schema)\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"sep\",\"\\t\")\n",
    "  .load(file_path)\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(table_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New section"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3742078295958070,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
